{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5abdbd5-97c6-46e5-ad53-de8e5a25a5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-26 14:44:23.298997: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag,word_tokenize\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "from tensorflow.keras.layers import Embedding,Dense,Flatten,concatenate,Input\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89ca24d-135f-4fb8-a43d-4091cac9ac1a",
   "metadata": {},
   "source": [
    "# Embedding Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b795958-d88e-451d-931d-4fe31961f0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self,vocab_size,embed_dim,pos_output_dim,max_len,pos_trainable_param):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.pos_output_dim=pos_output_dim\n",
    "        self.pos_input_dim = 20\n",
    "        self.max_len = max_len\n",
    "        self.char_to_int = {}\n",
    "        self.int_to_char ={}\n",
    "        self.pos_trainable_param = pos_trainable_param\n",
    "\n",
    "\n",
    "    def embed_sentences(self,word_index,model,trainable_param,X_train_pad):\n",
    "\n",
    "        embedding_matrix = np.zeros((self.vocab_size,self.embed_dim))\n",
    "        for word, i in word_index.items():\n",
    "            try:\n",
    "                embedding_vector = model[word]\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i]=embedding_vector\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        embed_layer = Embedding(self.vocab_size,self.embed_dim,weights =[embedding_matrix],trainable=trainable_param)\n",
    "\n",
    "        input_seq = Input(shape=(X_train_pad.shape[1],))\n",
    "        embed_seq = embed_layer(input_seq)\n",
    "        return input_seq,embed_seq\n",
    "\n",
    "    \n",
    "    def tag_pos1(self,sentences):\n",
    "        pos_tagged_sent = []\n",
    "        pos_tagged_sent_all = []\n",
    "        for sent in sentences:\n",
    "            pos_tagged_sent.extend(pos_tag(sent))\n",
    "            pos_tagged_sent_all.append(pos_tag(sent))\n",
    "        tags = list(set([i[1] for i in pos_tagged_sent]))\n",
    "        self.pos_input_dim = len(tags)\n",
    "        self.char_to_int = dict((c, i) for i, c in enumerate(tags))\n",
    "        self.int_to_char = dict((i, c) for i, c in enumerate(tags))\n",
    "\n",
    "        X_pos_encoded =[]\n",
    "        for i in range(len(pos_tagged_sent_all)):\n",
    "            temp = [self.char_to_int[pos[1]] for pos in pos_tagged_sent_all[i]]\n",
    "            X_pos_encoded.append(temp)\n",
    "\n",
    "        return np.array(X_pos_encoded)\n",
    "\n",
    "\n",
    "    def embed_pos(self,X_pos_arr):\n",
    "        input_seq_pos = Input(shape=(X_pos_arr.shape[1],))\n",
    "        embed_seq_pos = Embedding(self.pos_output_dim,self.pos_input_dim,input_length=self.max_len, dropout=0.2,trainable=self.pos_trainable_param)(input_seq_pos)\n",
    "\n",
    "        return input_seq_pos,embed_seq_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793b34a7-682f-451a-bfe2-ee183721c2cf",
   "metadata": {},
   "source": [
    "### Deep learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c20e280-7be8-4242-84ae-7032e6cfcfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_build(input_seq,input_seq_pos,embed_seq,embed_seq_pos,pad_train_x,X_pos_arr,train_y,\n",
    "                epochs,batch_size,pad_test_x,X_pos_test_arr,test_y):\n",
    "    print()\n",
    "    x = concatenate([embed_seq, embed_seq_pos])\n",
    "    x = Dense(256,activation =\"relu\")(x)\n",
    "    x = Flatten()(x)\n",
    "    preds = Dense(1,activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs=[input_seq, input_seq_pos], outputs=preds)\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "    model.fit([pad_train_x, X_pos_arr], train_y, epochs=epochs,batch_size=batch_size,\n",
    "              validation_data=([pad_test_x, X_pos_test_arr], test_y))\n",
    "\n",
    "    predictions = model.predict([pad_test_x, X_pos_test_arr])\n",
    "    predictions = [0 if i<0.5 else 1 for i in predictions]\n",
    "    print(\"Accuracy: \",accuracy_score(test_y,predictions))\n",
    "    print(\"Classification Report: \",classification_report(test_y,predictions))\n",
    "\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
