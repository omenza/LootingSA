{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57a4a2c5-9258-4393-849c-ebcce0a74df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abbey/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import fasttext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input,Embedding,Dense,Flatten\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "#https://github.com/PrashantRanjan09/Improved-Word-Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "678c6014-3f46-4b02-bd80-31152ee17a3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "data.txt cannot be opened for training!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mfasttext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_unsupervised\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mskipgram\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/fasttext/FastText.py:559\u001b[0m, in \u001b[0;36mtrain_unsupervised\u001b[0;34m(*kargs, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m a \u001b[38;5;241m=\u001b[39m _build_args(args, manually_set_args)\n\u001b[1;32m    558\u001b[0m ft \u001b[38;5;241m=\u001b[39m _FastText(args\u001b[38;5;241m=\u001b[39ma)\n\u001b[0;32m--> 559\u001b[0m \u001b[43mfasttext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    560\u001b[0m ft\u001b[38;5;241m.\u001b[39mset_args(ft\u001b[38;5;241m.\u001b[39mf\u001b[38;5;241m.\u001b[39mgetArgs())\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ft\n",
      "\u001b[0;31mValueError\u001b[0m: data.txt cannot be opened for training!"
     ]
    }
   ],
   "source": [
    "model = fasttext.train_unsupervised('data.txt', model='skipgram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a59955f-f04a-4d4e-a0c1-536824b0812b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_dict(json_set):\n",
    "    for k,v in json_set.items():\n",
    "        if v == \"True\":\n",
    "            json_set[k]= True\n",
    "        elif v == \"False\":\n",
    "            json_set[k]=False\n",
    "        else:\n",
    "            json_set[k]=v\n",
    "    return json_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcea532c-44cc-40ea-bd06-ac817c71b364",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model_params.json\", \"r\") as f:\n",
    "    model_params = json.load(f)\n",
    "model_params = json_to_dict(model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d820d9aa-fa06-4ca4-8e71-fa0456ff0548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(vocab_size,max_len):\n",
    "    \"\"\"\n",
    "        Loads the keras imdb dataset\n",
    "        Args:\n",
    "            vocab_size = {int} the size of the vocabulary\n",
    "            max_len = {int} the maximum length of input considered for padding\n",
    "        Returns:\n",
    "            X_train = tokenized train data\n",
    "            X_test = tokenized test data\n",
    "    \"\"\"\n",
    "    INDEX_FROM = 3\n",
    "\n",
    "    (X_train,y_train),(X_test,y_test) = imdb.load_data(num_words = vocab_size,index_from = INDEX_FROM)\n",
    "\n",
    "    return X_train,X_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92928885-841d-479c-80e9-99709c17a399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_word_vectors_imdb(X):\n",
    "    \"\"\"\n",
    "        Prepares the input\n",
    "        Args:\n",
    "            X = tokenized data\n",
    "        Returns:\n",
    "            sentences = {list} sentences containing words as tokens\n",
    "            word_index = {dict} word and its indexes in whole of imdb corpus\n",
    "            X = {list} = tokenized data (indexes)\n",
    "    \"\"\"\n",
    "    INDEX_FROM = 3\n",
    "    word_to_index = imdb.get_word_index()\n",
    "    word_to_index = {k:(v+INDEX_FROM) for k,v in word_to_index.items()}\n",
    "\n",
    "    word_to_index[\"<START>\"] =1\n",
    "    word_to_index[\"<UNK>\"]=2\n",
    "\n",
    "    index_to_word = {v:k for k,v in word_to_index.items()}\n",
    "\n",
    "    sentences_as_words = []\n",
    "    for i in range(len(X)):\n",
    "        temp = [index_to_word[ids] for ids in X[i]]\n",
    "        sentences_as_words.append(temp)\n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    word_indexes = tokenizer.word_index\n",
    "    \"\"\"\n",
    "    return sentences_as_words,word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba90b83-f2f7-45ba-aa0e-80190f8b6786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_word_vectors(X):\n",
    "    sentences_as_words=[]\n",
    "    word_to_index={}\n",
    "    count=1\n",
    "    for sent in X:\n",
    "        temp = sent.split()\n",
    "        sentences_as_words.append(temp)\n",
    "    for sent in sentences_as_words:\n",
    "        for word in sent:\n",
    "            if word_to_index.get(word,None) is None:\n",
    "                word_to_index[word] = count\n",
    "                count +=1\n",
    "    index_to_word = {v:k for k,v in word_to_index.items()}\n",
    "    sentences=[]\n",
    "    for i in range(len(sentences_as_words)):\n",
    "        temp = [word_to_index[w] for w in sentences_as_words[i]]\n",
    "        sentences.append(temp)\n",
    "\n",
    "\n",
    "    return sentences_as_words,sentences,word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decb564d-4b04-4da1-9878-042eedf5ac3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep_ELMo(train_x,train_y,test_x,test_y,max_len):\n",
    "\n",
    "    INDEX_FROM = 3\n",
    "    word_to_index = imdb.get_word_index()\n",
    "    word_to_index = {k:(v+INDEX_FROM) for k,v in word_to_index.items()}\n",
    "\n",
    "    word_to_index[\"<START>\"] =1\n",
    "    word_to_index[\"<UNK>\"]=2\n",
    "\n",
    "    index_to_word = {v:k for k,v in word_to_index.items()}\n",
    "\n",
    "    sentences=[]\n",
    "    for i in range(len(train_x)):\n",
    "        temp = [index_to_word[ids] for ids in train_x[i]]\n",
    "        sentences.append(temp)\n",
    "\n",
    "    test_sentences=[]\n",
    "    for i in range(len(test_x)):\n",
    "        temp = [index_to_word[ids] for ids in test_x[i]]\n",
    "        test_sentences.append(temp)\n",
    "\n",
    "    train_text = [' '.join(sentences[i][:max_len]) for i in range(len(sentences))]\n",
    "    train_text = np.array(train_text, dtype=object)[:, np.newaxis]\n",
    "    train_label = train_y.tolist()\n",
    "\n",
    "    test_text = [' '.join(test_sentences[i][:500]) for i in range(len(test_sentences))]\n",
    "    test_text = np.array(test_text , dtype=object)[:, np.newaxis]\n",
    "    test_label = test_y.tolist()\n",
    "\n",
    "    return train_text,train_label,test_text,test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7182d535-518d-4e0c-a021-3ea229f7b949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def building_word_vector_model(option,sentences,embed_dim,workers,window,y_train):\n",
    "    \"\"\"\n",
    "        Builds the word vector\n",
    "        Args:\n",
    "            type = {bool} 0 for Word2vec. 1 for gensim Fastext. 2 for Fasttext 2018.\n",
    "            sentences = {list} list of tokenized words\n",
    "            embed_dim = {int} embedding dimension of the word vectors\n",
    "            workers = {int} no. of worker threads to train the model (faster training with multicore machines)\n",
    "            window = {int} max distance between current and predicted word\n",
    "            y_train = y_train\n",
    "        Returns:\n",
    "            model = Word2vec/Gensim fastText/ Fastext_2018 model trained on the training corpus\n",
    "    \"\"\"\n",
    "    if option == 0:\n",
    "        print(\"Training a word2vec model\")\n",
    "        model = Word2Vec(sentences=sentences, size = embed_dim, workers = workers, window = window)\n",
    "        print(\"Training complete\")\n",
    "\n",
    "    elif option == 1:\n",
    "        print(\"Training a Gensim FastText model\")\n",
    "        model = FastText(sentences=sentences, size = embed_dim, workers = workers, window = window)\n",
    "        print(\"Training complete\")\n",
    "\n",
    "    elif option == 2:\n",
    "        print(\"Training a Fasttext model from Facebook Research\")\n",
    "        y_train = [\"__label__positive\" if i==1 else \"__label__negative\" for i in y_train]\n",
    "\n",
    "        with open(\"imdb_train.txt\",\"w\") as text_file:\n",
    "            for i in range(len(sentences)):\n",
    "                print(sentences[i],y_train[i],file = text_file)\n",
    "\n",
    "        model = fasttext.skipgram(\"imdb_train.txt\",\"model_ft_2018_imdb\",dim = embed_dim)\n",
    "        print(\"Training complete\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7969bc-7fa9-4fda-b32b-cb17cbd8b87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_input(X_train,X_test,maxlen):\n",
    "    \"\"\"\n",
    "        Pads the input upto considered max length\n",
    "        Args:\n",
    "            X_train = tokenized train data\n",
    "            X_test = tokenized test data\n",
    "        Returns:\n",
    "            X_train_pad = padded tokenized train data\n",
    "            X_test_pad = padded tokenized test data\n",
    "    \"\"\"\n",
    "\n",
    "    X_train_pad = pad_sequences(X_train,maxlen=maxlen,padding=\"post\")\n",
    "\n",
    "    X_test_pad = pad_sequences(X_test,maxlen=maxlen,padding=\"post\")\n",
    "\n",
    "    return X_train_pad,X_test_pad\n",
    "\n",
    "\n",
    "def ELMoEmbedding(x):\n",
    "    elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/1\", trainable=True)\n",
    "    return elmo_model(tf.squeeze(tf.cast(x, tf.string)), signature=\"default\", as_dict=True)[\"default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98b4921-e874-48e4-82b4-6ac61fc6e517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_model(embed_dim,X_train_pad,X_test_pad,y_train,y_test,vocab_size,word_index,w2vmodel,\n",
    "                         trainable_param,option):\n",
    "    \"\"\"\n",
    "        Builds the classification model for sentiment analysis\n",
    "        Args:\n",
    "            embded_dim = {int} dimension of the word vectors\n",
    "            X_train_pad = padded tokenized train data\n",
    "            X_test_pad = padded tokenized test data\n",
    "            vocab_size = {int} size of the vocabulary\n",
    "            word_index =  {dict} word and its indexes in whole of imdb corpus\n",
    "            w2vmodel = Word2Vec model\n",
    "            trainable_param = {bool} whether to train the word embeddings in the Embedding layer\n",
    "            option = {int} choice of word embedding\n",
    "    \"\"\"\n",
    "\n",
    "    embedding_matrix = np.zeros((vocab_size,embed_dim))\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_vector = w2vmodel[word]\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i]=embedding_vector\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    embed_layer = Embedding(vocab_size,embed_dim,weights =[embedding_matrix],trainable=trainable_param)\n",
    "\n",
    "    input_seq = Input(shape=(X_train_pad.shape[1],))\n",
    "    embed_seq = embed_layer(input_seq)\n",
    "    x = Dense(256,activation =\"relu\")(embed_seq)\n",
    "    x = Flatten()(x)\n",
    "    preds = Dense(1,activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(input_seq,preds)\n",
    "\n",
    "\n",
    "    model.compile(loss=model_params[\"loss\"],optimizer=model_params[\"optimizer\"],metrics= model_params[\"metrics\"])\n",
    "\n",
    "    model.fit(X_train_pad,y_train,epochs=model_params[\"epochs\"],batch_size=model_params[\"batch_size\"],validation_data=(X_test_pad,y_test))\n",
    "    predictions = model.predict(X_test_pad)\n",
    "    predictions = [0 if i<0.5 else 1 for i in predictions]\n",
    "    print(\"Accuracy: \",accuracy_score(y_test,predictions))\n",
    "    print(\"Classification Report: \",classification_report(y_test,predictions))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d942f13-66fc-413e-929f-bb0e92c1919c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Classification_model_with_ELMo(X_train_pad,y_train,X_test_pad,y_test):\n",
    "    input_text = layers.Input(shape=(1,), dtype=tf.string)\n",
    "    embed_seq = layers.Lambda(ELMoEmbedding, output_shape=(1024,))(input_text)\n",
    "    x = Dense(256,activation =\"relu\")(embed_seq)\n",
    "    preds = Dense(1,activation=\"sigmoid\")(x)\n",
    "    model = Model(input_text,preds)\n",
    "\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "\n",
    "    model.fit(X_train_pad,y_train,epochs=10,batch_size=512,validation_data=(X_test_pad,y_test))\n",
    "\n",
    "    predictions = model.predict(X_test_pad)\n",
    "    predictions = [0 if i<0.5 else 1 for i in predictions]\n",
    "    print(\"Accuracy: \",accuracy_score(y_test,predictions))\n",
    "    print(\"Classification Report: \",classification_report(y_test,predictions))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3b03cb-4888-410d-9985-777652a1ee8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
